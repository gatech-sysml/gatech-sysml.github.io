---
---

The System for AI Lab (SAIL) at Georgia Tech, led by Prof. Alexey Tumanov, specializes in advancing systems support and resource management for machine learning (ML) to democratize large-scale AI systems. Our research encompasses the entire AI infrastructure stack, from foundational system design to the development of efficient ML training and inference algorithms. By focusing on managing the complete ML lifecycle, SAIL aims to enhance accessibility and efficiency in AI technologies.

# Recent News

- üéâ Congratulations to Payman Behnam, Amey Agrawal, Alind Khare, and Dhruv Garg! Three papers accepted at ACM SIGOPS Operating Systems Review, July 2025.
- We are looking for contributors for our new inference engine [Vajra](https://project-vajra.github.io/). ‚ö°Ô∏è
- Our papers on common anti-patterns in LLm Inference systems evaluations is now on [Arxiv](https://arxiv.org/pdf/2507.09019).
- Prof. Tumanov recognized with the Spring [2025 CIOS Honor Roll](https://blog.ctl.gatech.edu/2025/06/11/spring-2025-honor-roll/) - third semester in a row.
- Congratulations to Prof. Alexey Tumanov for being recognized with the College of Computing [Outstanding Junior Faculty Research Award](https://issuu.com/gt-computing/docs/2025_coc_awards_booklet_v1) in the Spring of 2025!
- SuperServe accepted at NSDI'25! [Paper](https://www.usenix.org/conference/nsdi25/presentation/khare) | [Video](https://www.youtube.com/watch?v=rErbqtM7Lvc)
- Client Availability in Federated Learning presented at EuroMLSys'25, co-located with EuroSys '25. [Paper](https://euromlsys.eu/pdf/euromlsys25-42.pdf)
- Our papers on [Medha](https://arxiv.org/abs/2502.14051) for efficient multi-million context LLM inference and [Maya](https://arxiv.org/pdf/2503.20191) for optimizing deep learning training workloads are now public.
- Congratulations to Prof. Tumanov on being [awarded tenure](https://www.cc.gatech.edu/news/computing-celebrates-2025-faculty-promotion-and-tenure-cases) and promotion to the position of associate professor üéâ.
- RocketKV üöÄ, fast long-context inference with is now KV-cache compression is now on [Arxiv](https://arxiv.org/abs/2502.14051).
- [Dhruv Garg](/members/dhruv-garg) secures [CRNCH PhD Fellowship 2025](https://crnch.gatech.edu/phd-fellowships-awarded/) for his research in Federated Fine-tuning of LLMs.
- Mnemosyne, our paper on efficient inference up to 10M token context lengths is now [public](https://arxiv.org/abs/2409.17264).
- Our paper on DL training checkpoint compression system, [DynaQuant](https://arxiv.org/abs/2306.11800) has been accepted at SoCC'24.
- Metron üìê -- our LLM inference system benchmark is now [public](https://x.com/agrawalamey12/status/1812203186494837226).
- [DŒµpS](https://arxiv.org/abs/2407.06167) and [SuperFedNAS](https://arxiv.org/abs/2301.10879) have been accepted at ECCV'24.
- Congratulations to [Prof. Alexey Tumanov](/members/alexey-tumanov) for being awarded college of computing outstanding junior faculty teaching award.
- Sarathi-Serve ‚ò∏Ô∏è, our paper on efficient LLM inference has been accepted at OSDI'24.
- Vidur üë≥üèΩ, our paper on large scale LLM inference cluster simulation has been accepted at MLSys'24.
- [Payman Behnam](/members/payman-behnam) awarded [NVIDIA Graduate Fellowship 2024](https://blogs.nvidia.com/blog/graduate-research-fellowships-for-2024/) for advancing machine learning and systems with high-performance, low-latency, and energy-efficient hardware designs.
- [Payman Behnam](/members/payman-behnam) receives [Qualcomm Innovation Fellowship 2023](https://www.qualcomm.com/research/university-relations/innovation-fellowship/winners) for his work on Hardware-Software Co-Design for DNN inference systems.
- [Amey Agrawal](/members/amey-agrawal) secures [CRNCH PhD Fellowship 2023](https://crnch.gatech.edu/phd-fellowships-awarded/) for his research in LLM inference.
